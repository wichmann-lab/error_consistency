{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Core Experiments via Sequential Feature Selection\n",
    "\n",
    "We found that Robert's experiments imply very similar rankings of models, meaning that not all of them are necessary to get a good estimate of human-machine EC.\n",
    "\n",
    "In fact, one should probably invest trials differently: Rather than measuring more corruptions, one should do more trials per condition, to get stable values.\n",
    "\n",
    "Here, we compile a dataframe of n_samples x n_experiments, where n_experiments is 17, one column for each of Robert's experiments. The n_samples are bootstraps, with all models thrown into one bucket, i.e bootstrap i of model j is a sample, for all values of i and j. We scatter all of these points into a 17-dimensional space and want to find those dimensions that best reconstruct the full thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our bootstrap data\n",
    "standard_df = pd.read_parquet(\n",
    "    \"data/model_wise_bootstrapped_ecs_standard_1000.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "# Robert takes the average EC by first averaging within each experiment, then averaging across them.\n",
    "# (how you average within each experiment doesn't matter, because first conditions then humans = first humans then conditions = all at once)\n",
    "\n",
    "# take the average within each of the 17 experiments, like Robert does\n",
    "exp_mean_df = standard_df.groupby(\n",
    "    [\"bootstrap_id\", \"experiment\", \"model\"], observed=True, as_index=False\n",
    ").mean(numeric_only=True)\n",
    "\n",
    "display(exp_mean_df)\n",
    "\n",
    "# unfortunately we have a few NaNs\n",
    "nan_df = exp_mean_df[exp_mean_df[\"model-human-ec\"].isna()]\n",
    "display(nan_df)\n",
    "print(nan_df[\"model\"].unique())\n",
    "\n",
    "nan_combinations = exp_mean_df[exp_mean_df[\"model-human-ec\"].isna()][\n",
    "    [\"bootstrap_id\", \"model\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "df_filtered = exp_mean_df.merge(\n",
    "    nan_combinations, on=[\"bootstrap_id\", \"model\"], how=\"left\", indicator=True\n",
    ")\n",
    "exp_mean_df_no_nan = df_filtered[df_filtered[\"_merge\"] == \"left_only\"].drop(\n",
    "    columns=[\"_merge\"]\n",
    ")\n",
    "\n",
    "display(exp_mean_df_no_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform it to clean numpy array representation\n",
    "pivoted = exp_mean_df_no_nan.pivot(\n",
    "    columns=[\"experiment\"], index=[\"bootstrap_id\", \"model\"], values=\"model-human-ec\"\n",
    ").reset_index()\n",
    "display(pivoted)\n",
    "\n",
    "trimmed = pivoted.drop(columns=[\"bootstrap_id\", \"model\"])\n",
    "datasets = trimmed.columns\n",
    "idx_to_dataset = {i: ds for i, ds in enumerate(datasets)}\n",
    "data = trimmed.to_numpy(dtype=float)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn Sequential Feature Selector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Conduct a train-test-split\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scale data to zero-mean and unit std\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Estimator: train on subset of features, predict all 17 ---\n",
    "# Multi-output regression: input = subset of features, output = full vector\n",
    "estimator = LinearRegression()\n",
    "\n",
    "mses = []  # will contain the MSEs for plotting the drop\n",
    "exp_vars = []  # will contain the explained variances of each run\n",
    "ordered_features = []  # will contain the newly selected feature for every run\n",
    "\n",
    "features_last_it = []\n",
    "for n_features in range(1, X_train_scaled.shape[1]):\n",
    "\n",
    "    # --- Feature selector ---\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator,\n",
    "        n_features_to_select=n_features,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"neg_mean_squared_error\",  # We want to minimize reconstruction error\n",
    "        cv=3,  # Use cross-validation to avoid overfitting\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # --- Fit selector ---\n",
    "    sfs.fit(X_train_scaled, X_train_scaled)  # literally regressing X -> X\n",
    "\n",
    "    # --- Selected features ---\n",
    "    selected_features = sfs.get_support(indices=True)\n",
    "    new_feature = [f for f in selected_features if f not in features_last_it][0]\n",
    "    features_last_it = selected_features.copy()\n",
    "    ordered_features.append(new_feature)\n",
    "    print(f\"Selected feature indices: {selected_features}, new feature: {new_feature}\")\n",
    "\n",
    "    # --- Evaluate reconstruction on test set ---\n",
    "    X_train_reduced = X_train_scaled[:, selected_features]\n",
    "    X_test_reduced = X_test_scaled[:, selected_features]\n",
    "\n",
    "    # Refit on full training data\n",
    "    final_model = LinearRegression().fit(X_train_reduced, X_train_scaled)\n",
    "    X_test_reconstructed = final_model.predict(X_test_reduced)\n",
    "\n",
    "    # --- Compute reconstruction error ---\n",
    "    mse = mean_squared_error(X_test_scaled, X_test_reconstructed)\n",
    "    print(f\"Test reconstruction MSE: {mse:.4f}\")\n",
    "    mses.append(mse)\n",
    "\n",
    "    # --- Express this in terms of explained variance ---\n",
    "    score = explained_variance_score(\n",
    "        X_test_scaled, X_test_reconstructed, multioutput=\"variance_weighted\"\n",
    "    )\n",
    "    print(f\"Test reconstruction Explained Variance: {score:.4f}\")\n",
    "    exp_vars.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"MSE\": mses,\n",
    "        \"Explained Variance\": exp_vars,\n",
    "        \"feature_idx\": ordered_features,\n",
    "        \"experiment\": [idx_to_dataset[idx] for idx in ordered_features],\n",
    "        \"n_features\": np.arange(1, len(mses) + 1),\n",
    "    }\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plt.grid(axis=\"y\")\n",
    "sns.lineplot(data=df, x=\"n_features\", y=\"MSE\", ax=ax)\n",
    "sns.despine()\n",
    "ax.set_xlim(1, 16)\n",
    "ax.set_ylim(0, df[\"MSE\"].max())\n",
    "ax.set_xlabel(\"Selected Experiments\")\n",
    "ax.set_ylabel(\"Reconstruction MSE\")\n",
    "ax.set_xticks(ticks=np.arange(1, len(df) + 1), labels=df[\"experiment\"].tolist())\n",
    "ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sfs_result_mse.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plt.grid(axis=\"y\")\n",
    "sns.lineplot(data=df, x=\"n_features\", y=\"Explained Variance\", ax=ax)\n",
    "sns.despine()\n",
    "ax.set_xlim(1, 16)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(\"Selected Experiments\")\n",
    "ax.set_ylabel(\"Explained Variance\")\n",
    "ax.set_xticks(ticks=np.arange(1, len(df) + 1), labels=df[\"experiment\"].tolist())\n",
    "ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sfs_result_expvar.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
