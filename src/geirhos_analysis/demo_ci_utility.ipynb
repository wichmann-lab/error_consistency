{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating how significance of EC difference can be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure that updates to imported files are immediately available without restarting the kernel\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from scipy import stats\n",
    "\n",
    "# not using this here because finer control needed\n",
    "# font = {\"size\": 15}\n",
    "# matplotlib.rc(\"font\", **font)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils import fast_cohen, simulate_trials_from_copy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the bootstrapped data\n",
    "df = pd.read_parquet(\n",
    "    \"data/model_wise_bootstrapped_ecs_standard_1000.parquet\", engine=\"pyarrow\"\n",
    ")\n",
    "df[\"model\"] = df[\"model\"].astype(str)\n",
    "\n",
    "# take the mean over conditions\n",
    "exp_mean_df = df.groupby(\n",
    "    [\"bootstrap_id\", \"experiment\", \"model\"], observed=True, as_index=False\n",
    ").mean(numeric_only=True)\n",
    "\n",
    "# take the average across the experiments\n",
    "mean_df = exp_mean_df.groupby(\n",
    "    [\"bootstrap_id\", \"model\"], observed=True, as_index=False\n",
    ").mean(numeric_only=True)\n",
    "\n",
    "final_df = mean_df.groupby(\"model\")[\"model-human-ec\"].mean().reset_index()\n",
    "display(final_df)\n",
    "\n",
    "models = final_df.sort_values(by=\"model-human-ec\", ascending=False)[\"model\"].values\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approach\n",
    "Logic is: Select two models and humans, bootstrap (I have the data already) and compare the distribution of EC-deltas to the null hypothesis of no difference with a two-sided t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_value(df, m1, m2, plot=True):\n",
    "    \"\"\"\n",
    "    This calculates the p-value by doing a 1-sample t-test between the distribution of EC deltas and population mean 0.\n",
    "    The problem with this is that we get very small p-values simply because of doing so many bootstraps, but what this is neglecting is the noise\n",
    "    incurred by different participants etc (i.e. our sample is fixed).\n",
    "\n",
    "    I'm still keeping the code for future reference, and because the plotting is still useful.\n",
    "\n",
    "    :param m1: name of model 1\n",
    "    :param m2: name of model 2\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[(df[\"model\"].isin([m1, m2]))]\n",
    "\n",
    "    # printing the CIs\n",
    "    lower, upper = np.percentile(\n",
    "        mean_df[mean_df[\"model\"] == m1][\"model-human-ec\"].to_numpy(), [2.5, 97.5]\n",
    "    )\n",
    "    print(f\"95% interval for {m1}: [{lower:.3f}, {upper:.3f}]\")\n",
    "    lower, upper = np.percentile(\n",
    "        mean_df[mean_df[\"model\"] == m2][\"model-human-ec\"].to_numpy(), [2.5, 97.5]\n",
    "    )\n",
    "    print(f\"95% interval for {m2}: [{lower:.3f}, {upper:.3f}]\")\n",
    "\n",
    "    jdf = pd.merge(\n",
    "        left=mean_df[mean_df[\"model\"] == m1],\n",
    "        right=mean_df[mean_df[\"model\"] == m2],\n",
    "        on=[\"bootstrap_id\"],\n",
    "    )\n",
    "    jdf[\"ec-delta\"] = jdf[\"model-human-ec_x\"] - jdf[\"model-human-ec_y\"]\n",
    "\n",
    "    lower, upper = np.percentile(jdf[\"ec-delta\"].to_numpy(), [2.5, 97.5])\n",
    "    print(\n",
    "        f\"mean delta: {np.mean(jdf['ec-delta'].to_numpy()):.3f} 95% interval for delta: [{lower:.3f}, {upper:.3f}]\"\n",
    "    )\n",
    "\n",
    "    t_statistic, p_value = stats.ttest_1samp(jdf[\"ec-delta\"].to_numpy(), popmean=0)\n",
    "\n",
    "    if plot:\n",
    "        print(f\"t = {t_statistic:.3f}, p = {p_value}\")\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        sns.histplot(data=jdf, x=\"ec-delta\", ax=ax)\n",
    "        sns.despine()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax.grid(axis=\"both\")\n",
    "        sns.regplot(data=jdf, x=\"model-human-ec_x\", y=\"model-human-ec_y\", ax=ax)\n",
    "        mini = np.min([jdf[\"model-human-ec_x\"].min(), jdf[\"model-human-ec_x\"].min()])\n",
    "        maxi = np.max([jdf[\"model-human-ec_x\"].max(), jdf[\"model-human-ec_x\"].max()])\n",
    "        ax.plot(\n",
    "            np.linspace(mini, maxi, 100), np.linspace(mini, maxi, 100), linestyle=\":\"\n",
    "        )\n",
    "        sns.despine()\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = \"clip\"\n",
    "m1 = \"BiTM_resnetv2_101x1\"\n",
    "m2 = \"BiTM_resnetv2_152x2\"\n",
    "m3 = \"BiTM_resnetv2_50x1\"\n",
    "m4 = \"resnet50_l2_eps5\"\n",
    "calc_p_value(df, m0, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_p_value(df, m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_p_value(df, m2, m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating all p-values between adjacent models, using the broken method\n",
    "for idx, m1 in enumerate(models[:-2]):\n",
    "    m2 = models[idx + 1]\n",
    "    p = calc_p_value(df, m1, m2, False)\n",
    "    print(m1, m2, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Approach\n",
    "\n",
    "Logic is: Represent the null hypothesis of models having an equal EC by, for every human-model1-model2 triplet, randomly flipping (p=0.5) the model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "script_path = \"bootstrap_rebuttal.py\"\n",
    "\n",
    "for idx, m1 in enumerate(models[:-2]):\n",
    "    m2 = models[idx + 1]\n",
    "\n",
    "    command = [\"python\", script_path, \"-m1\", m1, \"-m2\", m2]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    print(f\"Command: {' '.join(command)}\")\n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    print(f\"Output:\\n{result.stdout}\")\n",
    "    if result.stderr:\n",
    "        print(f\"Errors:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_value_correctly(model1, model2, plot=False):\n",
    "    print(f\"Comparing {model1} and {model2}\")\n",
    "\n",
    "    null_hypo_df = pd.read_parquet(\n",
    "        f\"data/rebuttal_model_wise_bootstrapped_ecs_standard_1000_{model1}_{model2}.parquet\",\n",
    "        engine=\"pyarrow\",\n",
    "    )\n",
    "    null_hypo_df[\"model\"] = null_hypo_df[\"model\"].astype(str)\n",
    "\n",
    "    # take the mean over conditions\n",
    "    null_hypo_exp_mean_df = null_hypo_df.groupby(\n",
    "        [\"bootstrap_id\", \"experiment\", \"model\"], observed=True, as_index=False\n",
    "    ).mean(numeric_only=True)\n",
    "\n",
    "    # take the average across the experiments\n",
    "    null_hypo_mean_df = null_hypo_exp_mean_df.groupby(\n",
    "        [\"bootstrap_id\", \"model\"], observed=True, as_index=False\n",
    "    ).mean(numeric_only=True)\n",
    "\n",
    "    null_hypo_joined_df = pd.merge(\n",
    "        left=null_hypo_mean_df[null_hypo_mean_df[\"model\"] == model1],\n",
    "        right=null_hypo_mean_df[null_hypo_mean_df[\"model\"] == model2],\n",
    "        on=[\"bootstrap_id\"],\n",
    "    )\n",
    "    null_hypo_joined_df[\"ec-delta\"] = (\n",
    "        null_hypo_joined_df[\"model-human-ec_x\"]\n",
    "        - null_hypo_joined_df[\"model-human-ec_y\"]\n",
    "    )\n",
    "\n",
    "    lower, upper = np.percentile(\n",
    "        null_hypo_joined_df[\"ec-delta\"].to_numpy(), [2.5, 97.5]\n",
    "    )\n",
    "    print(\n",
    "        f\"mean delta under null: {np.mean(null_hypo_joined_df['ec-delta'].to_numpy()):.3f} 95% interval for delta: [{lower:.3f}, {upper:.3f}]\"\n",
    "    )\n",
    "\n",
    "    real_delta = null_hypo_joined_df[null_hypo_joined_df[\"bootstrap_id\"] == 0][\n",
    "        \"ec-delta\"\n",
    "    ].values[0]\n",
    "    print(\"real EC delta: \", real_delta)\n",
    "\n",
    "    all_deltas = np.abs(\n",
    "        null_hypo_joined_df[null_hypo_joined_df[\"bootstrap_id\"] > 0][\"ec-delta\"].values\n",
    "    )\n",
    "    larger_deltas = all_deltas > np.abs(real_delta)\n",
    "    proportion = np.mean(larger_deltas)\n",
    "    print(f\"Proportion of values under null more extreme than real_delta: {proportion}\")\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        sns.histplot(data=null_hypo_joined_df, x=\"ec-delta\", ax=ax)\n",
    "        sns.despine()\n",
    "\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1s = []\n",
    "model2s = []\n",
    "props = []\n",
    "\n",
    "for idx, model1 in enumerate(models[:-2]):\n",
    "    model2 = models[idx + 1]\n",
    "\n",
    "    prop = calc_p_value_correctly(model1, model2)\n",
    "\n",
    "    model1s.append(model1)\n",
    "    model2s.append(model2)\n",
    "    props.append(prop)\n",
    "\n",
    "res_df = pd.DataFrame({\"Model 1\": model1s, \"Model 2\": model2s, \"P Value\": props})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data for comparing only the second-best model to all others, sequentially\n",
    "\n",
    "import subprocess\n",
    "\n",
    "script_path = \"bootstrap_rebuttal.py\"\n",
    "\n",
    "m1 = models[1]\n",
    "for m2 in models[2:]:\n",
    "\n",
    "    command = [\"python\", script_path, \"-m1\", m1, \"-m2\", m2]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    print(f\"Command: {' '.join(command)}\")\n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    print(f\"Output:\\n{result.stdout}\")\n",
    "    if result.stderr:\n",
    "        print(f\"Errors:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1s = []\n",
    "model2s = []\n",
    "props = []\n",
    "\n",
    "model1 = models[1]\n",
    "for model2 in models[2:]:\n",
    "\n",
    "    prop = calc_p_value_correctly(model1, model2)\n",
    "\n",
    "    model1s.append(model1)\n",
    "    model2s.append(model2)\n",
    "    props.append(prop)\n",
    "\n",
    "other_res_df = pd.DataFrame({\"Model 1\": model1s, \"Model 2\": model2s, \"P Value\": props})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(other_res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
